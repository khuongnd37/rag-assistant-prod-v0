import re
import os
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

# Import PyPDF2 v·ªõi fallback handling
try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
    print("‚úÖ PyPDF2 ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
except ImportError:
    print("‚ö†Ô∏è PyPDF2 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. M·ªôt s·ªë t√≠nh nƒÉng PDF c√≥ th·ªÉ b·ªã h·∫°n ch·∫ø.")
    PYPDF2_AVAILABLE = False

# Import python-docx v·ªõi fallback handling
try:
    import docx
    DOCX_AVAILABLE = True
    print("‚úÖ python-docx ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
except ImportError:
    print("‚ö†Ô∏è python-docx ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Kh√¥ng th·ªÉ x·ª≠ l√Ω file DOCX.")
    DOCX_AVAILABLE = False

# Import LangChain v·ªõi fallback handling
try:
    from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.schema import Document
    LANGCHAIN_AVAILABLE = True
    print("‚úÖ LangChain ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
except ImportError:
    print("‚ö†Ô∏è LangChain ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. S·ª≠ d·ª•ng ch·∫ø ƒë·ªô x·ª≠ l√Ω c∆° b·∫£n.")
    LANGCHAIN_AVAILABLE = False
    
    # Fallback Document class khi LangChain kh√¥ng kh·∫£ d·ª•ng
    class Document:
        def __init__(self, page_content: str, metadata: Dict[str, Any] = None):
            self.page_content = page_content
            self.metadata = metadata or {}

# Thi·∫øt l·∫≠p logging system
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """
    X·ª≠ l√Ω t√†i li·ªáu to√†n di·ªán cho h·ªá th·ªëng RAG
    H·ªó tr·ª£ PDF, DOCX, TXT v·ªõi fallback strategies
    T·ªëi ∆∞u cho file l·ªõn v√† t√≠ch h·ª£p Streamlit
    """
    
    def __init__(self, 
                 chunk_size: int = None, 
                 chunk_overlap: int = None,
                 enable_smart_splitting: bool = True,
                 max_file_size_mb: int = 50):
        """
        Kh·ªüi t·∫°o DocumentProcessor v·ªõi c·∫•u h√¨nh t√πy ch·ªânh
        
        Args:
            chunk_size: K√≠ch th∆∞·ªõc chunk (m·∫∑c ƒë·ªãnh 1000 k√Ω t·ª±)
            chunk_overlap: ƒê·ªô tr√πng l·∫∑p gi·ªØa chunks (m·∫∑c ƒë·ªãnh 200 k√Ω t·ª±)
            enable_smart_splitting: B·∫≠t t√°ch chunk th√¥ng minh theo lo·∫°i t√†i li·ªáu
            max_file_size_mb: K√≠ch th∆∞·ªõc file t·ªëi ƒëa cho ph√©p (MB)
        """
        self.chunk_size = chunk_size or int(os.getenv("chunk_size", "1000"))
        self.chunk_overlap = chunk_overlap or int(os.getenv("chunk_overlap", "200"))
        self.enable_smart_splitting = enable_smart_splitting
        self.max_file_size_mb = max_file_size_mb
        
        # Patterns nh·∫≠n di·ªán lo·∫°i t√†i li·ªáu ƒë·ªÉ t·ªëi ∆∞u x·ª≠ l√Ω
        self.document_patterns = {
            'legal': [
                r"Ch∆∞∆°ng\s+[IVXLCDM]+", r"ƒêi·ªÅu\s+\d+", r"Lu·∫≠t\s+\w+",
                r"Ngh·ªã ƒë·ªãnh\s+\d+", r"Th√¥ng t∆∞\s+\d+", r"Quy·∫øt ƒë·ªãnh\s+\d+"
            ],
            'academic': [
                r"Abstract\s*:", r"T√≥m t·∫Øt\s*:", r"References\s*:",
                r"AI\s+", r"Machine Learning", r"Deep Learning", 
                r"Neural Network", r"Artificial Intelligence",
                r"Methodology\s*:", r"Research\s+", r"Algorithm\s*:",
                r"Data\s+Science", r"Computer\s+Vision", r"NLP"
            ],
            'business': [
                r"Executive Summary", r"Market Analysis", 
                r"Financial Projection", r"Business Plan"
            ]
        }
        
        self._setup_text_splitters()
        logger.info(f"‚úÖ DocumentProcessor kh·ªüi t·∫°o th√†nh c√¥ng: chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
    
    def _setup_text_splitters(self):
        """Thi·∫øt l·∫≠p c√°c text splitters chuy√™n bi·ªát cho t·ª´ng lo·∫°i t√†i li·ªáu"""
        if not LANGCHAIN_AVAILABLE:
            logger.warning("‚ö†Ô∏è LangChain kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p x·ª≠ l√Ω c∆° b·∫£n")
            return
        
        try:
            # Splitter cho t√†i li·ªáu h·ªçc thu·∫≠t/AI (t·ªëi ∆∞u cho file k·ªπ thu·∫≠t)
            self.academic_splitter = RecursiveCharacterTextSplitter(
                separators=[
                    r"\n#{1,6}\s",                     # Headers markdown
                    r"\n\d+\.\s",                     # Numbered sections
                    r"\n[A-Z][^\n]*:\s*\n",          # Section headers
                    r"\n\n",                          # Paragraphs
                    r"\n",                            # Lines
                    r"\. ",                           # Sentences
                    r" ",                             # Words
                    r""                               # Characters
                ],
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                is_separator_regex=True
            )
            
            # Splitter chu·∫©n cho t√†i li·ªáu th√¥ng th∆∞·ªùng
            self.standard_splitter = RecursiveCharacterTextSplitter(
                separators=[r"\n\n", r"\n", r"\. ", r" ", r""],
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len
            )
            
            logger.info("‚úÖ Text splitters ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p th√†nh c√¥ng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói thi·∫øt l·∫≠p text splitters: {e}")
            self.academic_splitter = None
            self.standard_splitter = None
    
    def _validate_file(self, file_path: str) -> bool:
        """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa file tr∆∞·ªõc khi x·ª≠ l√Ω"""
        try:
            path = Path(file_path)
            
            if not path.exists():
                logger.error(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}")
                return False
                
            if not path.is_file():
                logger.error(f"‚ùå ƒê∆∞·ªùng d·∫´n kh√¥ng ph·∫£i l√† file: {file_path}")
                return False
                
            file_size_mb = path.stat().st_size / (1024 * 1024)
            logger.info(f"üìÑ File: {path.name}")
            logger.info(f"üìä K√≠ch th∆∞·ªõc: {file_size_mb:.2f} MB")
            logger.info(f"üìã Lo·∫°i: {path.suffix}")
            
            if file_size_mb > self.max_file_size_mb:
                logger.warning(f"‚ö†Ô∏è File l·ªõn ({file_size_mb:.2f}MB > {self.max_file_size_mb}MB)")
                logger.warning("‚ö†Ô∏è Qu√° tr√¨nh x·ª≠ l√Ω c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian")
                
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói ki·ªÉm tra file {file_path}: {e}")
            return False
    
    def detect_document_type(self, content: str) -> str:
        """Ph√°t hi·ªán lo·∫°i t√†i li·ªáu d·ª±a tr√™n n·ªôi dung ƒë·ªÉ t·ªëi ∆∞u x·ª≠ l√Ω"""
        if not self.enable_smart_splitting:
            return 'standard'
        
        scores = {}
        for doc_type, patterns in self.document_patterns.items():
            score = sum(1 for pattern in patterns 
                       if re.search(pattern, content, re.IGNORECASE | re.MULTILINE))
            scores[doc_type] = score
        
        max_score = max(scores.values()) if scores else 0
        if max_score >= 1:
            detected_type = max(scores, key=scores.get)
            logger.info(f"üîç Ph√°t hi·ªán lo·∫°i t√†i li·ªáu: {detected_type} (ƒëi·ªÉm: {max_score})")
            return detected_type
        
        logger.info("üîç S·ª≠ d·ª•ng lo·∫°i t√†i li·ªáu chu·∫©n")
        return 'standard'
    
    def get_text_splitter(self, doc_type: str):
        """L·∫•y text splitter ph√π h·ª£p v·ªõi lo·∫°i t√†i li·ªáu"""
        if not LANGCHAIN_AVAILABLE:
            return None
        
        if doc_type == 'academic' and hasattr(self, 'academic_splitter'):
            return self.academic_splitter
        elif hasattr(self, 'standard_splitter'):
            return self.standard_splitter
        else:
            return None
    
    def _simple_chunk_split(self, text: str) -> List[str]:
        """T√°ch chunk ƒë∆°n gi·∫£n khi LangChain kh√¥ng kh·∫£ d·ª•ng"""
        if len(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            if end < len(text):
                # T√¨m ƒëi·ªÉm c·∫Øt t·ªët nh·∫•t ƒë·ªÉ tr√°nh c·∫Øt gi·ªØa t·ª´/c√¢u
                for separator in ['\n\n', '\n', '. ', ' ']:
                    break_point = text.rfind(separator, start, end)
                    if break_point > start:
                        end = break_point + len(separator)
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = max(end - self.chunk_overlap, start + 1)
        
        return chunks
    
    def process_pdf(self, 
                   file_path: str, 
                   use_langchain: bool = True,
                   parent_retriever: bool = True) -> List[Document]:
        """
        X·ª≠ l√Ω file PDF v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p fallback
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file PDF (tham s·ªë b·∫Øt bu·ªôc)
            use_langchain: ∆Øu ti√™n s·ª≠ d·ª•ng LangChain (khuy·∫øn ngh·ªã)
            parent_retriever: Tham s·ªë t∆∞∆°ng th√≠ch v·ªõi code c≈©
            
        Returns:
            List[Document]: Danh s√°ch document chunks ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
            
        Raises:
            ValueError: Khi file_path tr·ªëng ho·∫∑c file kh√¥ng c√≥ n·ªôi dung
            FileNotFoundError: Khi file kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng h·ª£p l·ªá
            ImportError: Khi kh√¥ng c√≥ th∆∞ vi·ªán PDF n√†o ƒë∆∞·ª£c c√†i ƒë·∫∑t
        """
        # Ki·ªÉm tra tham s·ªë b·∫Øt bu·ªôc
        if not file_path:
            raise ValueError("file_path l√† tham s·ªë b·∫Øt bu·ªôc v√† kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
        
        if not self._validate_file(file_path):
            raise FileNotFoundError(f"File kh√¥ng h·ª£p l·ªá ho·∫∑c kh√¥ng t·ªìn t·∫°i: {file_path}")
        
        try:
            pdf_path = Path(file_path)
            documents = []
            
            logger.info(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF: {pdf_path.name}")
            
            # Ph∆∞∆°ng ph√°p 1: S·ª≠ d·ª•ng LangChain (∆∞u ti√™n)
            if use_langchain and LANGCHAIN_AVAILABLE:
                logger.info("üìö S·ª≠ d·ª•ng LangChain PyPDFLoader...")
                loader = PyPDFLoader(str(pdf_path))
                pages = loader.load()
                
                if not pages:
                    raise ValueError(f"Kh√¥ng t√¨m th·∫•y n·ªôi dung trong PDF: {pdf_path}")
                
                logger.info(f"üìñ ƒê√£ t·∫£i {len(pages)} trang t·ª´ PDF")
                
                # Ph√¢n t√≠ch lo·∫°i t√†i li·ªáu t·ª´ m·∫´u n·ªôi dung
                sample_content = "\n".join([page.page_content[:1000] for page in pages[:5]])
                doc_type = self.detect_document_type(sample_content)
                
                # T√°ch chunks v·ªõi splitter ph√π h·ª£p
                text_splitter = self.get_text_splitter(doc_type)
                if text_splitter:
                    chunks = text_splitter.split_documents(pages)
                    logger.info(f"‚úÇÔ∏è ƒê√£ t·∫°o {len(chunks)} chunks b·∫±ng LangChain splitter")
                else:
                    chunks = pages
                    logger.info(f"‚úÇÔ∏è S·ª≠ d·ª•ng {len(chunks)} chunks t·ª´ pages g·ªëc")
                
                # X·ª≠ l√Ω v√† l√†m s·∫°ch chunks
                processed_count = 0
                for idx, chunk in enumerate(chunks):
                    # B·ªè qua chunks qu√° ng·∫Øn (c√≥ th·ªÉ l√† noise)
                    if len(chunk.page_content.strip()) < 50:
                        continue
                    
                    # Th√™m metadata chi ti·∫øt cho m·ªói chunk
                    source = chunk.metadata.get("source", str(pdf_path))
                    page = chunk.metadata.get("page", 0)
                    
                    chunk.metadata.update({
                        "id": f"{pdf_path.name}:{page}:{idx}",
                        "document_type": doc_type,
                        "chunk_index": idx,
                        "file_name": pdf_path.name,
                        "file_type": "pdf",
                        "processing_method": "langchain",
                        "page_number": page + 1,
                        "chunk_length": len(chunk.page_content)
                    })
                    
                    documents.append(chunk)
                    processed_count += 1
                
                logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω {processed_count} chunks h·ª£p l·ªá")
            
            # Ph∆∞∆°ng ph√°p 2: Fallback s·ª≠ d·ª•ng PyPDF2
            elif PYPDF2_AVAILABLE:
                logger.info("üìö S·ª≠ d·ª•ng PyPDF2 fallback...")
                with open(pdf_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    
                    all_text = ""
                    for page_num, page in enumerate(pdf_reader.pages):
                        try:
                            text = page.extract_text()
                            if text and text.strip():
                                all_text += text.strip() + "\n\n"
                        except Exception as page_error:
                            logger.warning(f"‚ö†Ô∏è L·ªói tr√≠ch xu·∫•t trang {page_num + 1}: {page_error}")
                            continue
                    
                    if not all_text.strip():
                        raise ValueError("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t text t·ª´ PDF")
                    
                    # Ph√°t hi·ªán lo·∫°i t√†i li·ªáu
                    doc_type = self.detect_document_type(all_text[:5000])
                    
                    # T√°ch chunks b·∫±ng ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n
                    chunk_texts = self._simple_chunk_split(all_text)
                    
                    for idx, chunk_text in enumerate(chunk_texts):
                        doc = Document(
                            page_content=chunk_text,
                            metadata={
                                'id': f"{pdf_path.name}:{idx}",
                                'chunk_index': idx,
                                'file_type': 'pdf',
                                'source': str(pdf_path),
                                'file_name': pdf_path.name,
                                'processing_method': 'pypdf2_simple',
                                'document_type': doc_type,
                                'chunk_length': len(chunk_text)
                            }
                        )
                        documents.append(doc)
            
            else:
                # Kh√¥ng c√≥ th∆∞ vi·ªán n√†o kh·∫£ d·ª•ng
                raise ImportError(
                    "Kh√¥ng c√≥ th∆∞ vi·ªán PDF n√†o ƒë∆∞·ª£c c√†i ƒë·∫∑t. "
                    "Vui l√≤ng c√†i ƒë·∫∑t m·ªôt trong c√°c th∆∞ vi·ªán sau:\n"
                    "- pip install PyPDF2\n"
                    "- pip install langchain-community pypdf"
                )
            
            if not documents:
                raise ValueError(f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung h·ª£p l·ªá t·ª´ PDF: {pdf_path}")
            
            logger.info(f"üéâ Ho√†n th√†nh x·ª≠ l√Ω PDF: {len(documents)} chunks t·ª´ {pdf_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω PDF {file_path}: {str(e)}")
            raise
    
    def process_docx(self, file_path: str, use_langchain: bool = True) -> List[Document]:
        """X·ª≠ l√Ω file DOCX v·ªõi fallback strategies"""
        if not file_path:
            raise ValueError("file_path l√† tham s·ªë b·∫Øt bu·ªôc v√† kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
        
        if not self._validate_file(file_path):
            raise FileNotFoundError(f"File kh√¥ng h·ª£p l·ªá: {file_path}")
        
        try:
            docx_path = Path(file_path)
            documents = []
            
            logger.info(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω DOCX: {docx_path.name}")
            
            if use_langchain and LANGCHAIN_AVAILABLE:
                # S·ª≠ d·ª•ng LangChain Docx2txtLoader
                logger.info("üìö S·ª≠ d·ª•ng LangChain Docx2txtLoader...")
                loader = Docx2txtLoader(str(docx_path))
                pages = loader.load()
                
                if not pages:
                    raise ValueError(f"Kh√¥ng t√¨m th·∫•y n·ªôi dung trong DOCX: {docx_path}")
                
                full_content = pages[0].page_content
                doc_type = self.detect_document_type(full_content[:5000])
                
                text_splitter = self.get_text_splitter(doc_type)
                if text_splitter:
                    chunks = text_splitter.split_documents(pages)
                    logger.info(f"‚úÇÔ∏è ƒê√£ t·∫°o {len(chunks)} chunks b·∫±ng LangChain splitter")
                else:
                    chunks = pages
                    logger.info(f"‚úÇÔ∏è S·ª≠ d·ª•ng {len(chunks)} chunks t·ª´ pages g·ªëc")
                
                processed_count = 0
                for idx, chunk in enumerate(chunks):
                    if len(chunk.page_content.strip()) < 50:
                        continue
                    
                    chunk.metadata.update({
                        "id": f"{docx_path.name}:{idx}",
                        "document_type": doc_type,
                        "chunk_index": idx,
                        "file_name": docx_path.name,
                        "file_type": "docx",
                        "processing_method": "langchain",
                        "chunk_length": len(chunk.page_content)
                    })
                    documents.append(chunk)
                    processed_count += 1
                
                logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω {processed_count} chunks h·ª£p l·ªá")
            
            elif DOCX_AVAILABLE:
                # Fallback s·ª≠ d·ª•ng python-docx
                logger.info("üìö S·ª≠ d·ª•ng python-docx fallback...")
                doc = docx.Document(docx_path)
                
                all_text = ""
                for paragraph in doc.paragraphs:
                    if paragraph.text.strip():
                        all_text += paragraph.text.strip() + "\n\n"
                
                if not all_text.strip():
                    raise ValueError("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t text t·ª´ DOCX")
                
                doc_type = self.detect_document_type(all_text[:5000])
                chunk_texts = self._simple_chunk_split(all_text)
                
                for idx, chunk_text in enumerate(chunk_texts):
                    if len(chunk_text.strip()) < 50:
                        continue
                    
                    doc_obj = Document(
                        page_content=chunk_text,
                        metadata={
                            'id': f"{docx_path.name}:{idx}",
                            'chunk_index': idx,
                            'file_type': 'docx',
                            'source': str(docx_path),
                            'file_name': docx_path.name,
                            'processing_method': 'python_docx',
                            'document_type': doc_type,
                            'chunk_length': len(chunk_text)
                        }
                    )
                    documents.append(doc_obj)
            
            else:
                raise ImportError(
                    "Kh√¥ng c√≥ th∆∞ vi·ªán DOCX n√†o ƒë∆∞·ª£c c√†i ƒë·∫∑t. "
                    "Vui l√≤ng c√†i ƒë·∫∑t: pip install python-docx ho·∫∑c pip install langchain-community docx2txt"
                )
            
            if not documents:
                raise ValueError(f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ DOCX: {docx_path}")
            
            logger.info(f"üéâ Ho√†n th√†nh: {len(documents)} chunks t·ª´ {docx_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω DOCX {file_path}: {str(e)}")
            raise
    
    def process_txt(self, file_path: str, encoding: str = 'utf-8', use_langchain: bool = True) -> List[Document]:
        """X·ª≠ l√Ω file TXT v·ªõi multiple encoding support"""
        if not file_path:
            raise ValueError("file_path l√† tham s·ªë b·∫Øt bu·ªôc v√† kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
        
        if not self._validate_file(file_path):
            raise FileNotFoundError(f"File kh√¥ng h·ª£p l·ªá: {file_path}")
        
        try:
            txt_path = Path(file_path)
            documents = []
            
            logger.info(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω TXT: {txt_path.name}")
            
            if use_langchain and LANGCHAIN_AVAILABLE:
                # S·ª≠ d·ª•ng LangChain TextLoader v·ªõi fallback encoding
                logger.info("üìö S·ª≠ d·ª•ng LangChain TextLoader...")
                
                pages = None
                encodings_to_try = [encoding, 'utf-8', 'cp1252', 'latin-1', 'ascii']
                
                for enc in encodings_to_try:
                    try:
                        loader = TextLoader(str(txt_path), encoding=enc)
                        pages = loader.load()
                        logger.info(f"‚úÖ ƒê√£ ƒë·ªçc file v·ªõi encoding: {enc}")
                        break
                    except UnicodeDecodeError:
                        continue
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è L·ªói v·ªõi encoding {enc}: {e}")
                        continue
                
                if not pages:
                    raise ValueError(f"Kh√¥ng th·ªÉ ƒë·ªçc file v·ªõi b·∫•t k·ª≥ encoding n√†o: {txt_path}")
                
                full_content = pages[0].page_content
                doc_type = self.detect_document_type(full_content[:5000])
                
                text_splitter = self.get_text_splitter(doc_type)
                if text_splitter:
                    chunks = text_splitter.split_documents(pages)
                    logger.info(f"‚úÇÔ∏è ƒê√£ t·∫°o {len(chunks)} chunks b·∫±ng LangChain splitter")
                else:
                    chunks = pages
                    logger.info(f"‚úÇÔ∏è S·ª≠ d·ª•ng {len(chunks)} chunks t·ª´ pages g·ªëc")
                
                processed_count = 0
                for idx, chunk in enumerate(chunks):
                    if len(chunk.page_content.strip()) < 50:
                        continue
                    
                    chunk.metadata.update({
                        "id": f"{txt_path.name}:{idx}",
                        "document_type": doc_type,
                        "chunk_index": idx,
                        "file_name": txt_path.name,
                        "file_type": "txt",
                        "processing_method": "langchain",
                        "chunk_length": len(chunk.page_content)
                    })
                    documents.append(chunk)
                    processed_count += 1
                
                logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω {processed_count} chunks h·ª£p l·ªá")
            
            else:
                # Fallback ƒë·ªçc file tr·ª±c ti·∫øp v·ªõi multiple encoding
                logger.info("üìö S·ª≠ d·ª•ng ƒë·ªçc file TXT c∆° b·∫£n...")
                
                all_text = None
                encodings_to_try = [encoding, 'utf-8', 'cp1252', 'latin-1', 'ascii']
                
                for enc in encodings_to_try:
                    try:
                        with open(txt_path, 'r', encoding=enc, errors='ignore') as file:
                            all_text = file.read()
                        logger.info(f"‚úÖ ƒê√£ ƒë·ªçc file v·ªõi encoding: {enc}")
                        break
                    except UnicodeDecodeError:
                        continue
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è L·ªói v·ªõi encoding {enc}: {e}")
                        continue
                
                if not all_text or not all_text.strip():
                    raise ValueError("File TXT tr·ªëng ho·∫∑c kh√¥ng th·ªÉ ƒë·ªçc")
                
                # Detect document type
                doc_type = self.detect_document_type(all_text[:5000])
                
                # T√°ch chunks
                chunk_texts = self._simple_chunk_split(all_text)
                
                for idx, chunk_text in enumerate(chunk_texts):
                    if len(chunk_text.strip()) < 50:
                        continue
                    
                    doc = Document(
                        page_content=chunk_text,
                        metadata={
                            'id': f"{txt_path.name}:{idx}",
                            'chunk_index': idx,
                            'file_type': 'txt',
                            'source': str(txt_path),
                            'file_name': txt_path.name,
                            'processing_method': 'basic_txt',
                            'document_type': doc_type,
                            'chunk_length': len(chunk_text)
                        }
                    )
                    documents.append(doc)

            if not documents:
                raise ValueError(f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ TXT: {txt_path}")
            
            logger.info(f"üéâ Ho√†n th√†nh: {len(documents)} chunks t·ª´ {txt_path.name}")
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω TXT {file_path}: {str(e)}")
            raise
    
    def process_document(self, file_path: str, **kwargs) -> List[Document]:
        """X·ª≠ l√Ω t√†i li·ªáu t·ª± ƒë·ªông theo extension c·ªßa file"""
        if not file_path:
            raise ValueError("file_path l√† tham s·ªë b·∫Øt bu·ªôc v√† kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
        
        file_path_obj = Path(file_path)
        extension = file_path_obj.suffix.lower()
        
        logger.info(f"üîÑ ƒêang x·ª≠ l√Ω file: {file_path_obj.name} ({extension})")
        
        if extension == '.pdf':
            return self.process_pdf(str(file_path_obj), **kwargs)
        elif extension == '.docx':
            return self.process_docx(str(file_path_obj), **kwargs)
        elif extension == '.txt':
            return self.process_txt(str(file_path_obj), **kwargs)
        else:
            logger.error(f"‚ùå Kh√¥ng h·ªó tr·ª£ lo·∫°i file: {extension}")
            supported_formats = ['.pdf', '.docx', '.txt']
            raise ValueError(f"Lo·∫°i file '{extension}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. C√°c ƒë·ªãnh d·∫°ng h·ªó tr·ª£: {supported_formats}")
    
    def get_statistics(self, documents: List[Document]) -> Dict[str, Any]:
        """T·∫°o th·ªëng k√™ v·ªÅ c√°c document ƒë√£ x·ª≠ l√Ω"""
        if not documents:
            return {
                'total_documents': 0,
                'total_characters': 0,
                'average_chunk_size': 0,
                'file_types': {},
                'document_types': {},
                'min_chunk_size': 0,
                'max_chunk_size': 0,
                'processing_methods': {}
            }
        
        chunk_lengths = [len(doc.page_content) for doc in documents]
        
        stats = {
            'total_documents': len(documents),
            'total_characters': sum(chunk_lengths),
            'average_chunk_size': sum(chunk_lengths) / len(documents),
            'file_types': {},
            'document_types': {},
            'processing_methods': {},
            'min_chunk_size': min(chunk_lengths),
            'max_chunk_size': max(chunk_lengths)
        }
        
        for doc in documents:
            # ƒê·∫øm file types
            file_type = doc.metadata.get('file_type', 'unknown')
            stats['file_types'][file_type] = stats['file_types'].get(file_type, 0) + 1
            
            # ƒê·∫øm document types
            doc_type = doc.metadata.get('document_type', 'unknown')
            stats['document_types'][doc_type] = stats['document_types'].get(doc_type, 0) + 1
            
            # ƒê·∫øm processing methods
            method = doc.metadata.get('processing_method', 'unknown')
            stats['processing_methods'][method] = stats['processing_methods'].get(method, 0) + 1
        
        return stats

# H√†m ti·ªán √≠ch t∆∞∆°ng th√≠ch v·ªõi code c≈© (wrapper function)
def load_and_split_pdf(pdf_path: str, parent_retriver: bool = True) -> List[Document]:
    """
    H√†m wrapper t∆∞∆°ng th√≠ch v·ªõi code c≈©, s·ª≠ d·ª•ng DocumentProcessor ƒë·ªÉ x·ª≠ l√Ω PDF.
    
    Args:
        pdf_path: ƒê∆∞·ªùng d·∫´n file PDF.
        parent_retriver: Tham s·ªë t∆∞∆°ng th√≠ch (kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng tr·ª±c ti·∫øp trong logic n√†y).
        
    Returns:
        List[Document]: Danh s√°ch c√°c chunks t√†i li·ªáu.
        
    Raises:
        ValueError: Khi pdf_path tr·ªëng
        Exception: C√°c l·ªói kh√°c t·ª´ qu√° tr√¨nh x·ª≠ l√Ω
    """
    if not pdf_path:
        raise ValueError("pdf_path kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
    
    try:
        processor = DocumentProcessor(
            chunk_size=int(os.getenv("chunk_size", "1000")),
            chunk_overlap=int(os.getenv("chunk_overlap", "200")),
            max_file_size_mb=100
        )
        # Truy·ªÅn r√µ r√†ng tham s·ªë file_path
        return processor.process_pdf(
            file_path=pdf_path,
            parent_retriever=parent_retriver
        )
    except Exception as e:
        logger.error(f"‚ùå L·ªói trong load_and_split_pdf: {e}")
        raise

# Factory function ƒë·ªÉ t·∫°o processor (helper function)
def create_document_processor(chunk_size: int = 1000, 
                            chunk_overlap: int = 200,
                            max_file_size_mb: int = 100) -> DocumentProcessor:
    """
    Factory function ƒë·ªÉ t·∫°o m·ªôt th·ªÉ hi·ªán c·ªßa DocumentProcessor v·ªõi c√°c tham s·ªë m·∫∑c ƒë·ªãnh
    ho·∫∑c t√πy ch·ªânh.
    
    Args:
        chunk_size: K√≠ch th∆∞·ªõc chunk m·∫∑c ƒë·ªãnh.
        chunk_overlap: ƒê·ªô tr√πng l·∫∑p chunk m·∫∑c ƒë·ªãnh.
        max_file_size_mb: K√≠ch th∆∞·ªõc file t·ªëi ƒëa m·∫∑c ƒë·ªãnh.
        
    Returns:
        DocumentProcessor: M·ªôt th·ªÉ hi·ªán m·ªõi c·ªßa DocumentProcessor.
    """
    return DocumentProcessor(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        enable_smart_splitting=True,
        max_file_size_mb=max_file_size_mb
    )

# H√†m ti·ªán √≠ch ƒë·ªÉ x·ª≠ l√Ω batch files
def process_multiple_documents(file_paths: List[str], 
                             processor: DocumentProcessor = None,
                             **kwargs) -> Dict[str, Any]:
    """
    X·ª≠ l√Ω nhi·ªÅu t√†i li·ªáu c√πng l√∫c
    
    Args:
        file_paths: Danh s√°ch ƒë∆∞·ªùng d·∫´n files
        processor: DocumentProcessor instance (t·∫°o m·ªõi n·∫øu None)
        **kwargs: Tham s·ªë b·ªï sung cho processor
        
    Returns:
        Dict ch·ª©a k·∫øt qu·∫£ x·ª≠ l√Ω v√† th·ªëng k√™
    """
    if processor is None:
        processor = create_document_processor()
    
    results = {
        'successful': [],
        'failed': [],
        'all_documents': [],
        'statistics': {}
    }
    
    for file_path in file_paths:
        try:
            documents = processor.process_document(file_path, **kwargs)
            results['successful'].append({
                'file_path': file_path,
                'document_count': len(documents),
                'documents': documents
            })
            results['all_documents'].extend(documents)
            logger.info(f"‚úÖ Th√†nh c√¥ng: {file_path} - {len(documents)} chunks")
            
        except Exception as e:
            results['failed'].append({
                'file_path': file_path,
                'error': str(e)
            })
            logger.error(f"‚ùå Th·∫•t b·∫°i: {file_path} - {e}")
    
    # T·∫°o th·ªëng k√™ t·ªïng h·ª£p
    if results['all_documents']:
        results['statistics'] = processor.get_statistics(results['all_documents'])
    
    return results

# H√†m ƒë·ªÉ ki·ªÉm tra kh·∫£ nƒÉng x·ª≠ l√Ω file
def check_file_support(file_path: str) -> Dict[str, Any]:
    """
    Ki·ªÉm tra xem file c√≥ th·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω kh√¥ng
    
    Args:
        file_path: ƒê∆∞·ªùng d·∫´n file c·∫ßn ki·ªÉm tra
        
    Returns:
        Dict ch·ª©a th√¥ng tin v·ªÅ kh·∫£ nƒÉng x·ª≠ l√Ω file
    """
    path = Path(file_path)
    extension = path.suffix.lower()
    
    support_info = {
        'file_path': file_path,
        'file_name': path.name,
        'extension': extension,
        'exists': path.exists(),
        'is_file': path.is_file() if path.exists() else False,
        'size_mb': 0,
        'supported': False,
        'recommended_method': None,
        'available_methods': []
    }
    
    if support_info['exists'] and support_info['is_file']:
        support_info['size_mb'] = path.stat().st_size / (1024 * 1024)
    
    # Ki·ªÉm tra h·ªó tr·ª£ theo extension
    if extension == '.pdf':
        support_info['supported'] = True
        if LANGCHAIN_AVAILABLE:
            support_info['recommended_method'] = 'langchain'
            support_info['available_methods'].append('langchain')
        if PYPDF2_AVAILABLE:
            support_info['available_methods'].append('pypdf2')
            if not support_info['recommended_method']:
                support_info['recommended_method'] = 'pypdf2'
                
    elif extension == '.docx':
        support_info['supported'] = True
        if LANGCHAIN_AVAILABLE:
            support_info['recommended_method'] = 'langchain'
            support_info['available_methods'].append('langchain')
        if DOCX_AVAILABLE:
            support_info['available_methods'].append('python-docx')
            if not support_info['recommended_method']:
                support_info['recommended_method'] = 'python-docx'
                
    elif extension == '.txt':
        support_info['supported'] = True
        if LANGCHAIN_AVAILABLE:
            support_info['recommended_method'] = 'langchain'
            support_info['available_methods'].append('langchain')
        support_info['available_methods'].append('basic')
        if not support_info['recommended_method']:
            support_info['recommended_method'] = 'basic'
    
    return support_info

# H√†m main ƒë·ªÉ test
def main():
    """H√†m main ƒë·ªÉ test c√°c ch·ª©c nƒÉng c·ªßa DocumentProcessor"""
    print("üöÄ Testing DocumentProcessor...")
    
    # Test kh·ªüi t·∫°o
    processor = DocumentProcessor(
        chunk_size=1000,
        chunk_overlap=200,
        max_file_size_mb=50
    )
    print(f"‚úÖ Processor ƒë√£ kh·ªüi t·∫°o: chunk_size={processor.chunk_size}")
    
    # Test file support check
    test_files = ["test.pdf", "test.docx", "test.txt", "test.unknown"]
    print("\nüìã Ki·ªÉm tra h·ªó tr·ª£ file:")
    for file_path in test_files:
        support = check_file_support(file_path)
        status = "‚úÖ H·ªó tr·ª£" if support['supported'] else "‚ùå Kh√¥ng h·ªó tr·ª£"
        print(f"  {file_path}: {status} - {support['available_methods']}")
    
    print("\nüéâ Test ho√†n th√†nh!")

if __name__ == "__main__":
    main()
